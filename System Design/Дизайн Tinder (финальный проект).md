## Scope Refinement
Будет реализован только функционал создания и заполнения профиля, поиска и выбора пары, а также функционал общения в чате. Фичи связанные с премиальными подписками и их оплатой, различные механики, специфичные для конкретных сервисов, по типу суперлайков, подборов пары по результатам анкетирования, звонки и прочее реализованы не будут.

## Functional Requirements
1. Создание профиля, добавление в него фотографий (до 10 на профиль), заполнение описания и выбор интересов.
2. Отображение клиентам ленты из кандидатов, которым можно поставить дизлайк или лайк для потенциального создания пары.
3. Если два пользователя поставили друг другу взаимные лайки, у них должна создаваться пара. 
4. Для новосозднной пары должен создаваться чат для общения. О новых сообщениях должны приходить алерты.
5. При получении нового сообщения в чате или при создании новой пары, должно приходить уведомление.

Дополнительные требования:
6. Должна быть реализована система, которая подбирает ленту рекомендаций на основе предпочтений, интересов и характеристик пользователя.
7. Составлять ленту рекомендаций с учетом геолокации пользователей (подбирать для пользователя потенциальных партнеров, находящихся в каком-то адекватном радиусе от него). 

## Non-functional Requirements
1. Высокая отзывчивость системы: 
   - при входе в приложение сразу же должна появляться лента с людьми, со схожими интересами/характеристиками.
   - при входе в чаты должны сразу же отображаться последние сообщения собеседников и история беседы.
2. Высокая доступность: приложение должно быть доступно в любое время дня и ночи по всему миру.
3. Географическая распределенность.

## Раздел с оценкой нагрузки новой системы;
Сервис представлен на рынке практически всех стран мира.
MAU = 100M, DAU = 10M.

Далее прикидка нагрузки делается без учета возможных пиков, так как предполагается, что проектируемый сервис уже вышел на плато по ажиотажу.

### Расчётные показатели по загрузке
Скорее всего имеет смысл отдельно прикинуть нагрузку на:
- **Просмотр ленты**.  Заложим предпосылку, что средний пользователь Tinder просматривает 100 профилей в день.
- **Мэтчи**. Также заложим, что у среднего пользователя в Tinder происходит 2 мэтча в день (иначе он потеряет интерес заходить в приложение).
- **Общение в чате**. Наконец, предположим, что средний пользователь Tinder пишет и читает ~500 сообщений в день.

Сделаем допущение, что в сутках 100 000 секунд, а не 86 400.

Таким образом нагрузка будет вычислена следующим образом:
- **Reads**: просмотры ленты + чтение сообщений = 
  (100 + 500) * 10M / 100 000 сек = 60 000 RPS
- **Writes**: свайпы + мэтчи + отправка сообщений = 
  (100 + 2 + 500) * 10M / 100 000 сек = 60 200 RPS

По такой прикидке получается, что нагрузка на чтение и запись примерно равна и существенного перекоса не наблюдается.

### Сетевой трафик
Из описанной ранее нагрузки можно сделать вывод, что основная нагрузка на сетевой трафик будет происходить за счет просмотра фотографий и написания сообщений в чате.

При расчете сетевого трафика будем руководствоваться следующими предпосылками:
- средний вес картинки = 500 KB
- средний вес сообщения = 200 B
- запись свайпа/матча = 1 KB

Предположим, что пользователи в среднем принимают решение о свайпе по результатам просмотра одной фотографии в секунду.

Тогда средний сетевой трафик составит:
10 000 RPS (на фото) * 500KB + 50 000 RPS (на сообщения) * 200 B = 5 GB/s (округлим до 5 GB/s).

В сутки это будет:
5 GB/s × 100 000 секунд в сутках = 500 000 GB = 500 TB/день.

В год это будет:
500 TB * 400 дней = 200 PB/год.
### Нагрузка на хранилище

Среднее количество операций в день:
- Свайпы: 100 × 10 M = 1 000 000 000
- Мэтчи: 2 × 10 M = 20 000 000
- Сообщения: 500 × 10 M = 5 000 000 000

Нагрузка на хранилище складывается из:
- Свайпы + мэтчи: 1 000 000 000 + 20 000 000 ≈ 1 020 000 000 KB ≈ 1 020 GB ≈ 1 TB.
- Сообщения: 5 000 000 000 × 200 B = 1 000 000 000 000 B ≈ 1 TB.
- Фото (просмотры/загрузки): 1 000 000 000 × 500 KB = 500 000 000 000 KB ≈ 500 TB.

Итого хранилище в день: 500 TB + 1 TB + 1 TB ≈ 502 TB/день.

С учетом необходимости добавления реплик можно смело умножать емкость хранилища на 3, то есть получится 1.5 PB/день.  Также для перестраховки можно заложить 10% на выход из строя каких-то серверов: 1.5 * 0.1 ≈  1.7 PB/день.

### Нагрузка на вычислительные мощности
Очевидно, что для хранения данных чатов, профилей, и фотографий компании следует использовать облачные хранилища, поэтому тут стоит задумываться не о вычислительных мощностях, а об издержках на поддержание инфраструктуры у облачного провайдера (AWS, Google, Cloudflare).

### Стоимость
Тут я затрудняюсь привести конкретные прикидки, но осмелюсь предположить, что при учете вводных по DAU и MAU, а также видах нагрузки, описанных выше, стоимость реализации такого сервиса будет порядка нескольких миллиардов долларов.


## Дизайн системы
В системе предполагается одна категория пользователей.
Для того, чтобы пользовали регистрировались в приложении, создавали и обновляли свои анкеты предусмотрен `User service`. Текстовые формализованные анкетные данные профилей будут храниться в реляционной БД с индексами (так как чтение из этой БД будет превышать запись). Фото пользователей, а также описания их профилей в свободной форме будут храниться в документном S3-подобном хранилище.

Пользователям должна отображаться заранее подготовленная лента рекомендаций, она будет готовиться в `Recommendation feed service`. Важно, чтобы лента рекомендаций отображалась пользователю максимально быстро, поэтому ее целесообразно хранить в быстром key-value хранилище. Сами рекомендации будут готовиться во внешнем `Recomendation service`, который будет поддерживаться ML-командой. `Recomendation service` должен иметь доступ к данным о свайпах, мэтчах, а также пользовательских анкетах.

Также сервис рекомендаций должен учитывать геолокацию пользователей при подготовке данных для ленты рекомендаций. В этой связи сервис рекомендаций будет обращаться к сервису для поиска ближайших кандидатов `GeoSearch service`. 
Геоданные с кординатами должны писаться в отдельную колоночную БД. Отдельный сервис `Indexing service` должен создавать для них гео-хэши.

Все свайпы и мэтчи клиентов должны регистрироваться через сервис `Swipes/Matches service` и публиковаться в очередь сообщений, чтобы потом эти ивенты вычитал `Notification service` и отправил через внешний партнерский сервис алерты о создании пары. Сами данные о мэтчах и свайпах можно хранить в key-value БД, так как они именют фиксированную структуру, достаточно легковесны и обращение к ним происходит достаточно часто. Архивные данные о мэтчах можно хранить в отдельном холодном колоночном хранилище.

Для общения между пользователями предусмотрен `Chat service`. Он будет складывать сообщения в колоночную БД и забирать данные из нее же, но через кэш (по типу Cassandra). Соединение и обмен сообщениями между пользователями может осуществляться через WebSocket (как отразить это на схеме – хз).
Также сервис с чатом будет отправлять ивенты в очередь сообщений, чтобы сервис `Notification service` вычитал их и отправил все необходимые алерты.

Далее, чтобы не загромождать схему я описываю, какие элементы необходимо добавить в систему, чтобы соблюсти нефункциональные требования:
- **балансировщики нагрузки**, чтобы распределять нагрузку равномерно по инстансам сервисов и тем самым обеспечить доступность;
- **ограничители нагрузки**, чтобы защитить систему от злоумышленников и ботов;
- **множество инстансов сервисов**, для увеличения их отзывчивости, производительности и отказоустойчивости;
- **прослойки в виде инстансов с кэшем**, чтобы повысить отзывчивость и уменьшить нагрузку на хранилища;
- **множество инстансов баз** и хранилищ, чтобы хранить в них распределенные шарды данных, что обеспечит масштабируемость и итоговую консистентность;
- **несколько реплик баз** и хранилищ на случай отказа части из них.
- бэкапы для всех балансировщиков, ограничителей и кластеров очередей сообщений;
- для **мониторинга основных метрик сервисов** все сервисы пишут свои логи в очередь сообщений, `Logger service` их вычитывает и отдает в Grafana для построения дэшборд;
- все пользователи, пытающиеся провзаимодействовать с приложением должны попадать в него через **Gateway**, все нежелательные запросы к частям системы должны быть ограничены **файрволом**;
- **географическую распределенность** обеспечит CDN облачного провайдера.

![[Tinder Design.png]]