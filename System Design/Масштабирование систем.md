 Масштабирование систем достигается не только за счет **инвестиций в железо**, но и за счет **алгоритмов оптимизации** процессов.

Так или иначе в конечном итоге любая популярная система будет **распределенной**.

Самый простой способ масштабирования системы – увеличение количество систем (такой способ подходит, если между экземплярами системы не требуется синхронизация).

Мастштабированию подвержены различные компоненты системы:
- Сервисы можно балансировать за счет увеличения количества их инстансов и добавления балансировщика нагрузки.
- Хранилища также можно масштабировать за счет увеличения количества инстансов и проведения партиционирования данных с применением репликации данных.

Пример дизайна абстрактного масштабированного приложения:
![[Scaled application.png|600]]
## Балансировщик нагрузки (load balancer)
**Балансировщик нагрузки** (**load balancer**) – это компонент распределенной системы, который используется для сбалансированного распределения [[Расчёт нагрузки на систему|нагрузки]] (по [[Масштабирование систем#Алгоритмы работы балансировщиков нагрузки|определенному алгоритму]]) между различными:
- пользовательскими приложениями (в примере ниже загрузчики видео), 
- сервисами (в примере ниже обработчики видео), 
- хранилищами (в примере ниже хранилища видео).  

Для максимальной надежности балансировщики могут располагаться между каждой парой модулей системы:
![[Load balancer.png|600]]

Балансировщик также должен **следить за статусом жизнеспособности** сервисов, в которые он передает запросы, чтобы не передавать запросы в сервисы, вышедшие из строя.

В ситуациях, где нет необходимости в синхронном ответе между модулями системы можно установить [[Очереди сообщений|очередь сообщений]] вместо балансировщика нагрузки:
![[Que instead of load balancer.png|450]]

Преимущества балансировщика нагрузки:
- **улучшается пользовательский опыт** за счет снижения задержки (**latency**) при взаимодействии с системой;
- **снижается нагрузка** на каждый из серверов за счет распределения запросов между ними;
- **справляемся с выходом из строя** части серверов за счет перераспределения нагрузки между оставшимися;
- умные балансировщики умеют **прогнозировать рост нагрузки** и увеличивать количество серверов для обработки запросов (например, рост нагрузки до 80% – это знак, что нужно добавить еще один сервер в кластер).

В ходе своей работы балансировщик нагрузки решает следующие задачи:
1. Проверяет жизнеспособность серверов в кластере (**health check**). Не ответившим серверам дается шанс исправиться через 5/10/30 минут  (**exponential backoff**).
2. По [[Масштабирование систем#Алгоритмы работы балансировщиков нагрузки|определенному алгоритму]] выбирает подходящий сервер, на который будет распределена нагрузка.

### Алгоритмы работы балансировщиков нагрузки
Существует несколько критериев выбора сервера при распределении нагрузки:
- **Количество соединений**. Выбирается тот сервер, к которому в текущий момент установлено  наименьшее количество подключений.
- **Время ответа**. Выбирается сервер с наименьшей задержкой (**latency**) перед ответом.
- **Трафик**. Выбирается сервер с наименьшей нагрузкой на сетевой канал. 
- **Round-robin.** При отправке запросов сервера выбираются по очереди (по кругу).
- **Weighted round-robin**. Если в кластере мощность серверов разная, есть смысл назначить для каждого сервера свой вес, в соответствии с которым на него будет направляться нагрузка.
- **Пользовательский хэш**. Сервер выбирается на основе хэша по IP или user_id клиента. Таким образом все данные одного пользователя будут храниться на одном сервере.

## Распределение данных между хранилищами (партиционирование)
**Партиционирование** – процесс разделения больших хранимых объектов БД (файлы, таблицы, индексы) на логические части по определенным критериям. Партиционирование обеспечивает **масштабирование** и **производительность** системы.

Партиционирование бывает двух видов:
1. **Вертикальное**. Разные таблицы одной БД целиком раскладываются по разным серверам.
2. **Горизонтальное** (**sharding**). По определенному ключу (дата, user_id) таблица разбивается на сегменты (шарды), которые раскладываются по разным серверам. При обращении к данным запрос пользователя направляется на тот сервер, где хранится шард с данными, соответсвующий его идентификатору.

Способы горизонтального партиционирования:
- **Хэш-функция**. Можно взять какой-то атрибут (user_id/order_id), посчитать его хэш (например, по функции MD5), затем перевести хэш в численный вид, взять какой-нибудь mod (например, делим на 10) и в зависимости от остатка от деления поместить запись на тот или иной сервер (один из 10). 
	- В некоторых ситуациях возможен перегруз какого-то сервера из-за **проблемы селебрити** (например, записи активности по твитам Илона Маска). 
	- Также может быть **перекос нагрузки** из-за плохого выбора ключа партиционирования (например, таймстемп в качестве ключа для распределения записей о бронированиях в ресторане). 
	- Главный минус этого способа – при выпадении одного из серверов начнется переход в нагрузке. При добавлении новых серверов нужно осуществлять перераспределение данных.
- **По спискам**. Можно заранее разделить поступающие запросы по определенным группам (например, страны или дни недели) и распределять нагрузку по серверам соответствующим каждой из групп.
- **Round-robin**/**weighted round-robin**. Можно определять сервер для записи нагрузки по очереди по кругу.
- **Комбинированный подход**. Можно комбинировать вышеописанные подходы (например, разбить нагрузку по континентам и при этом использовать хэш-функцию).

Проблемы с горизонтальным партиционированием данных через шардирование:
- Из-за распределения данных по разным серверам могут возникнуть проблемы с удовлетворением [[Принципы ACID|требованиям ACID]] и выполнением JOINов. Проблема частично решается денормализацией (пропадает необходимость в JOINах).
- Может возникнуть проблема с использованием и **отслеживанием валидности внешних ключей** на данные в таблицах подвергнутым шардированию (если в пользовательской таблице есть ссылка на фотографию и фотография удалена, то невозможно своевременно отслеживать валидность ссылки).
- Если потребуется изменить подход к шардированию (если с текущим начались проблемы), может потребоваться **ребалансировка**. Ребалансировка приведет к **недоступности системы** на время ребалансировки данных между серверами.
## Избыточность и репликации
**Избыточность** – это дублирование каких-то компонентов системы для того, чтобы в случае выхода из строя компонента, его инстансы смогли подхватить его функционал.
Избыточность обеспечивает **увеличение надежности** системы.

Так, например, можно добавить избыточный инстанс балансировщика нагрузки, который будет находиться в режиме ожидания и при этом обмениваться информацией о состоянии (**health check**) с основным балансировщиком. В случае выхода из строя основного балансировщика он подменит основной, а основной в момент восстановления примет роль страхующего.

**Репликация** – это процесс хранения копий данных на разных серверах хранилища. Другими словами репликация – это копирование данных на избыточные инстансы.

Репликация повышает
- **надежность** системы, 
- **отказоустойчивость**
- и **доступность** (одна из реплик может отрабатывать запросы на запись, а другие запросы на чтение, за счет чего повышается доступность).

**Фактор репликации** – количество копий объекта данных (таблицы/шарда) на разных серверах. При записи нового блока файлов в хранилище он будет записан сразу же на то кол-во серверов, которое определено в факторе репликации. Если один из серверов выйдет из строя, то данные не потеряются, так как их копии сохранены на других серверах.

### Способы соблюдения консистентности при репликации
Существует два способа обеспечения консистентности (синхронизации) при репликации данных по различным серверам:
1. **Кворум**. Чтобы понять, что данные хранящиеся во множестве реплик все еще актуальны, необходимо, чтобы выполнялось правило **R + W > N** (выполняется кворум), где R – кол-во реплик пригодных для чтения, W – кол-во реплик пригодных для записи, N – общее кол-во реплик. При удовлетворении этому условию хотя бы на одной из реплик можно будет считать актуальные свеже записанные данные.
2. **Лидер и последователи**. Этот подход аналогичен тому, который используется в Kafka. Один из серверов выступает в роли лидера и осуществляет реплицирование данных, другие сервера выступают в роли последователей. Последователи записывают данные вслед за лидером, служат резервными копиями и помогают распределять нагрузку при запросах на чтение данных. Если лидер выходит из строя, один из последователей становится на его место.

### Консистентное хэширование
**Консистентное хэширование** – это подход в рамках которого вместо того, чтобы создавать реплики по количеству физических серверов, создается множество маленьких виртуальных реплик, равномерно разделенных между серверами.

В рамках этого подхода весь возможный диапазон хэшей (от 0 до 2<sup>32</sup>-1) представляется в виде кольца, на котором равномерно размещаются сервера (вычисляется хэш от их наименования). Каждому серверу соответствует своя точка на кольце:
![[Consistent hashing 1]]
Для каждой записи вычисляется хэш, после чего запись размещается на окружность и попадает на тот сервер, который находится справа от нее по часовой стрелке. Таким образом:
- user_id 101 c хэшом 228 попадет на сервер A
- user_id 102 c хэшом 1200 попадет на сервер B
- user_id 103 c хэшом 6750 попадет на сервер C

При **выходе из строя** какого-то сервера записи, находящиеся на нем, перемещаются на следующий сервер по часовой стрелке. Так, например, при падении сервера С, запись с user_id 103 (хэш которой 6750) будет перераспределена на сервер D.

При этом надо понимать, что для обеспечения равномерности распределения данных (чтобы избежать перекоса нагрузки) сервера **размещаются на окружности множество раз**. Так серверу A могут соответствовать точки 1000, 7000, 12000. Эти точки как раз и являются виртуальными репликами, соответствующими конкретному серверу.

Если не использовать консистенотное хэширование, то в случае отказа одного из серверов (или наоборот добавлении нового) приходится перестраивать логику реплицирования, что приведет к **массовому перераспределению** данных между серверами.



